
# λ‹¤μΈµ νΌμ…‰νΈλ΅ (MLP)μ—μ„ μμΉ λ―Έλ¶„μ΄ μ—­μ „νλ¥Ό λ€μ²΄ν•  μ μμ„κΉ?
---
**μ—­μ „ν μ•κ³ λ¦¬μ¦ μ—†μ΄ λ”¥λ¬λ‹ λ¨λΈμ„ λ§λ“¤ μ μμ„κΉ?**

μ •λ‹µμ€ "κ°€λ¥ν•λ‹¤". μ΄λ² κΈ€μ—μ„λ” **μ—­μ „ν**(Backpropagation)λ¥Ό μ‚¬μ©ν• λ¨λΈκ³Ό **μμΉ λ―Έλ¶„**(Numerical Differentiation)μ„ μ‚¬μ©ν• λ¨λΈ, λ‘ κ°€μ§€ λ²„μ „μ λ‹¤μΈµ νΌμ…‰νΈλ΅ (MLP) λ¨λΈμ„ λ°‘λ°”λ‹¥λ¶€ν„° μ§μ ‘ κµ¬ν„ ν•  κ²ƒμ΄λ‹¤. κµ¬ν„ν• λ¨λΈλ΅ MNIST μ†κΈ€μ”¨ μ«μ λ°μ΄ν„°μ…‹μ„ ν•™μµν•μ—¬ μ΄λ―Έμ§€λ¥Ό μ¬λ°”λ¥΄κ² λ¶„λ¥ν•΄λ ¤κ³  ν•λ‹¤.


**κΈ€μ„ μ½κΈ° μ „ μ•μ•„μ•Ό ν•  μ **

- μ΄ κΈ€μ—μ„λ” λ”¥λ¬λ‹ λ¨λΈμ„ μ²μλ¶€ν„° μ§μ ‘ κµ¬ν„ν•λ” κ²ƒμ΄ λ©ν‘μ΄λ―€λ΅, **Python**, **Numpy**, **Matplotlib**λ§ μ‚¬μ©ν•λ‹¤.
- μν•™μ  κ°λ…μ΄λ‚ μ•κ³ λ¦¬μ¦μ„ κΉμ΄ μκ² λ‹¤λ£¨μ§€λ” μ•λ”λ”λ‹¤.

## **μ—­μ „ν**
κΈ°κ³„ ν•™μµμ—μ„ [μ—­μ „ν(Backpropagation)](https://en.wikipedia.org/wiki/Backpropagation)λ” μ‹ κ²½λ§μ„ ν•™μµμ‹ν‚¤κΈ° μ„ν•΄ μ‚¬μ©λλ” κ³„μ‚°λ²•μ΄λ‹¤. μ΄ λ°©λ²•μ€ [μ—°μ‡„ λ²•μΉ™(Chain Rule)](https://en.wikipedia.org/wiki/Chain_rule)μ„ κΈ°λ°μΌλ΅ μ‘λ™ν•λ‹¤. μ—­μ „νμ λ©ν‘λ” μμ „νμ—μ„ μ‚¬μ©λ κ°€μ¤‘μΉ(weight)μ™€ νΈν–¥(bias)μ„ μ—…λ°μ΄νΈν•λ” κ²ƒμ΄λ‹¤. κ°€μ¤‘μΉμ™€ νΈν–¥μ΄ μ ν•™μµλ μλ΅ λ¨λΈμ μ •ν™•λ„κ°€ ν–¥μƒ λλ‹¤.
μ΄λ¬ν• νλΌλ―Έν„°λ¥Ό μ—…λ°μ΄νΈν•λ ¤λ©΄ κ° κ°€μ¤‘μΉμ™€ νΈν–¥μ κΈ°μΈκΈ°(gradient)λ¥Ό κ³„μ‚°ν•΄μ•Ό ν•λ”λ°, μ΄λ• κΈ°μΈκΈ°λ” κ° νλΌλ―Έν„°κ°€ μμΈ΅κ°’κ³Ό μ‹¤μ κ°’μ μ°¨μ΄(μ¤μ°¨)μ— μ–Όλ§λ‚ κΈ°μ—¬ν–λ”μ§€λ¥Ό λ‚νƒ€λ‚Έλ‹¤.

μ¤μ°¨λ¥Ό κ³„μ‚°ν• ν›„ λ‘ κ°€μ§€ μ„ νƒμ΄ μλ”λ°,

1. **μ—­μ „ν**λ¥Ό μ‚¬μ©ν•μ—¬ κ°€μ¤‘μΉλ¥Ό μ—­λ°©ν–¥μΌλ΅ μ „νν•κ³  κ° κ°€μ¤‘μΉμ™€ νΈν–¥μ κΈ°μΈκΈ°λ¥Ό κ³„μ‚°ν•λ” λ°©λ²•.
2. **μμΉ λ―Έλ¶„**μ„ μ‚¬μ©ν•μ—¬ νλΌλ―Έν„°(κ°€μ¤‘μΉμ™€ νΈν–¥)λ¥Ό μ•„μ£Ό μ΅°κΈ λ³€κ²½ν•κ³ , μ΄μ— λ”°λ¥Έ μ¤μ°¨μ λ³€ν™”λ¥Ό κ΄€μ°°ν•λ” λ°©λ²•.

μ΄λ² κµ¬ν„μ—μ„λ” **μ—­μ „ν**λ¥Ό μ‚¬μ©ν•μ§€ μ•κ³  **μμΉ λ―Έλ¶„**μΌλ΅ λ¨λΈμ„ ν•™μµμ‹μΌ λ³΄μ!

## μμΉ λ―Έλ¶„μ΄λ€

### λ―Έλ¶„μ μ •μ

κ³ λ“±ν•™κµμ—μ„ λ°°μ΄ λ―Έλ¶„μ μ •μλ¥Ό λ– μ¬λ ¤ λ³΄μ.

$$
f'(x) = \lim_{h \to 0} \frac{f(x+h) - f(x)}{h}
$$

- **h**: λ§¤μ° μ‘μ€ κ°’
- **h**κ°€ 0μ— κ°€κΉμ›μ§μλ΅ ν•΄λ‹Ή μ§€μ μ—μ„μ μκ°„ λ³€ν™”μ¨μ„ κµ¬ν•  μ μλ‹¤.

### μμΉ λ―Έλ¶„

μ‹¤μ  λ¬Έμ μ—μ„ κΈ°μΈκΈ°λ¥Ό μ •ν™•ν•κ² κ³„μ‚°ν•λ” κ²ƒμ€ μ–΄λ µκΈ° λ•λ¬Έμ—, μ°λ¦¬λ” **μμΉ λ―Έλ¶„**μ„ μ‚¬μ©ν•λ‹¤.

**μμΉ λ―Έλ¶„μ μΆ…λ¥:**

- **μ „λ°© μ°¨λ¶„(Forward Difference)**:

$$
\frac{f(x+h) - f(x)}{h} 
$$

- **ν›„λ°© μ°¨λ¶„(Backward Difference)**:

$$
\frac{f(x) - f(x-h)}{h} 
$$

- **μ¤‘μ•™ μ°¨λ¶„(Central Difference, λ” μ •ν™•ν•¨)**:

$$
\frac{f(x+h) - f(x-h)}{2h} 
$$

μ°λ¦¬λ” κ°€μ¥ μ •ν™•ν• λ°©λ²•μΈ **μ¤‘μ•™ μ°¨λ¶„**μ„ μ‚¬μ©ν•  κ²ƒμ΄λ‹¤.

### μμΉ λ―Έλ¶„μ„ λ¨λΈ ν•™μµμ— μ μ©ν•λ” λ°©λ²•

- $f(x+h)$ : κ° νλΌλ―Έν„° `x`μ— μ‘μ€ κ°’ `h`λ¥Ό λ”ν• ν›„ λ¨λΈμ μ¤μ°¨λ¥Ό κ³„μ‚°ν•λ‹¤.
- $f(x-h)$ : κ° νλΌλ―Έν„° `x`μ—μ„ μ‘μ€ κ°’ `h`λ¥Ό λΊ€ ν›„ λ¨λΈμ μ¤μ°¨λ¥Ό κ³„μ‚°ν•λ‹¤.
- $\frac{f(x+h) - f(x-h)}{2h}$ : λ‘ κ°’μ„ λ”ν• ν›„ $2h$λ΅ λ‚λ„λ©΄ κΈ°μΈκΈ°κ°€ λλ‹¤.
- μ΄ κ³Όμ •μ„ λ¨λ“  νλΌλ―Έν„°μ— λ€ν•΄ λ°λ³µν•μ—¬ κΈ°μΈκΈ°λ¥Ό κ³„μ‚°ν•ν•λ‹¤.

## μ‹¤μ  μ½”λ“μ— μ μ©ν•΄λ³΄μ (feat. MNIST)

### MNIST λ°μ΄ν„°μ…‹

- MNIST λ°μ΄ν„°μ…‹μ€ μ†κΈ€μ”¨ μ«μλ΅ μ΄λ£¨μ–΄μ§„ λ°μ΄ν„°μ…‹μ΄λ©°, ν•™μµμ© 60,000κ°, ν…μ¤νΈμ© 10,000κ°μ μ΄λ―Έμ§€λ΅ κµ¬μ„±λμ–΄ μλ‹¤.


![image.png](/posts/1/image_0.png)

### λ¨λΈ ν•™μµ π¤–

- **μμΉ λ―Έλ¶„**μ ν•µμ‹¬ κ°λ…μ€ μ†μ‹¤ ν•¨μμ κΈ°μΈκΈ°λ¥Ό μμΉμ μΌλ΅ κ³„μ‚°ν•λ” κ²ƒμ΄λ‹¤.
- κ°€μ¤‘μΉμ™€ νΈν–¥ κ°κ°μ— λ€ν•΄ μ•„μ£Ό μ‘μ€ κ°’ `h`λ¥Ό λ”ν•κ³  λΉΌλ©΄μ„ κΈ°μΈκΈ°λ¥Ό κ³„μ‚°ν•λ‹¤.
- μλ¥Ό λ“¤μ–΄, κ°€μ¤‘μΉ **W1**μ ν¬κΈ°κ°€ (784, 50)μ΄λΌλ©΄, λ¨λ“  μ”μ†μ— λ€ν•΄ `h`λ¥Ό μ¶”κ°€/μ κ±°ν•λ©° κΈ°μΈκΈ°λ¥Ό κµ¬ν•΄μ•Ό ν•λ―€λ΅ **39,200λ²**(784 x 50)μ κ³„μ‚°μ΄ ν•„μ”ν•λ‹¤.

**μ½”λ“ μμ‹**

```python
def numerical_gradient(f, y_true, x_data, params, h=1e-4):
    gradients = {}  # κ° νλΌλ―Έν„°(W, b)μ— λ€ν• κΈ°μΈκΈ°λ¥Ό μ €μ¥ν•  λ”•μ…”λ„λ¦¬

    for key in params:  # κ° νλΌλ―Έν„°(W1, W2, b1, b2)μ— λ€ν•΄ λ°λ³µ
        param = params[key]  # ν„μ¬ νλΌλ―Έν„° μ„ νƒ
        grad = np.zeros_like(param)  # νλΌλ―Έν„°μ™€ λ™μΌν• ν•νƒμ κΈ°μΈκΈ° λ°°μ—΄ μƒμ„±

        if param.ndim == 1:  # νΈν–¥ λ²΅ν„°μ κ²½μ°
            for i in range(param.shape[0]):  # κ° μ”μ†μ— λ€ν•΄ λ°λ³µ
                temp_params = params.copy()
                param_plus = param.copy()
                param_minus = param.copy()

                param_plus[i] += h  # b_iμ— h μ¶”κ°€
                param_minus[i] -= h  # b_iμ—μ„ h κ°μ†

                # νΈν–¥μ„ λ³€κ²½ν• μƒνƒμ—μ„ μ†μ‹¤ κ³„μ‚°
                temp_params[key] = param_plus
                loss_plus = f(y_true, x_data, temp_params)  
                temp_params[key] = param_minus
                loss_minus = f(y_true, x_data, temp_params)  

                grad[i] = (loss_plus - loss_minus) / (2 * h)  # μ¤‘μ•™ μ°¨λ¶„ λ°©μ‹μΌλ΅ κΈ°μΈκΈ° κ³„μ‚°

        else:  # κ°€μ¤‘μΉ ν–‰λ ¬μ κ²½μ°
            for i in range(param.shape[0]):  # ν–‰μ„ λ”°λΌ λ°λ³µ
                for j in range(param.shape[1]):  # μ—΄μ„ λ”°λΌ λ°λ³µ
                    temp_params = params.copy()
                    param_plus = param.copy()
                    param_minus = param.copy()

                    param_plus[i, j] += h  # w_ijμ— h μ¶”κ°€
                    param_minus[i, j] -= h  # w_ijμ—μ„ h κ°μ†

                    # κ°€μ¤‘μΉλ¥Ό λ³€κ²½ν• μƒνƒμ—μ„ μ†μ‹¤ κ³„μ‚°
                    temp_params[key] = param_plus
                    loss_plus = f(y_true, x_data, temp_params)  
                    temp_params[key] = param_minus
                    loss_minus = f(y_true, x_data, temp_params) 

                    grad[i, j] = (loss_plus - loss_minus) / (2 * h)  # μ¤‘μ•™ μ°¨λ¶„ λ°©μ‹μΌλ΅ κΈ°μΈκΈ° κ³„μ‚°

        gradients[key] = grad  # κ³„μ‚°λ κΈ°μΈκΈ° μ €μ¥

    return gradients  # λ¨λ“  κΈ°μΈκΈ° λ°ν™


```
## ν…μ¤νΈ κ²°κ³Ό

### μ‚¬μ „ μ¤€λΉ„

- λ¨λΈ = λ‹¤μΈµ νΌμ…‰νΈλ΅  (MLP)
- μ—ν­ν­ = 50
- λ°°μΉ ν¬κΈ° = 100
- λ μ΄μ–΄ μ = 2 (μ…λ ¥μΈµ, μ¶λ ¥μΈµ)
- νλΌλ―Έν„° = κ°€μ¤‘μΉ 2κ°, νΈν–¥ 2κ°
- μ†μ‹¤ ν•¨μ = ν¬λ΅μ¤ μ—”νΈλ΅ν”Ό
- ν™μ„±ν™” ν•¨μ = μ‹κ·Έλ¨μ΄λ“

### μ†μ‹¤ κ°’

![image.png](/posts/1/image_1.png)

### ν…μ¤νΈ μ •ν™•λ„

![image.png](/posts/1/image_2.png)

| λ°©λ²• | μ—ν¬ν¬ β³ | μ†μ” μ‹κ°„ β±οΈ | μ†μ‹¤ π“‰ | ν…μ¤νΈ μ •ν™•λ„ π― |
| --- | --- | --- | --- | --- |
| **μ—­μ „ν (Backpropagation)** | 100 | **3.1μ΄** | 1.6676 | 0.6783 |
| **μμΉ λ―Έλ¶„ (Numerical Differentiation)** | 100 | **141λ¶„ 55.3μ΄** | 1.5788 | 0.6309 |

## κ²°λ΅ 

μμΉ λ―Έλ¶„μ„ μ‚¬μ©ν•΄μ„ μ†μ‹¤ κΈ°μΈκΈ°λ¥Ό κ³„μ‚°ν•λ©΄ 2μΈµ λ¨λΈμ„ ν•™μµν•λ” λ° **μ—„μ²­λ‚ μ‹κ°„**μ΄ κ±Έλ¦°λ‹¤λ” κ±Έ μ• μ μλ‹¤. μ΄ λ°©λ²•μ€ **μ—­μ „νλ³΄λ‹¤ ν›¨μ”¬ λλ¦¬μ§€λ§**, λ³µμ΅ν• μµμ ν™” κΈ°λ²• μ—†μ΄λ„ μµμ ν™”κ°€ μ–΄λ–»κ² λ™μ‘ν•λ”μ§€ μ΄ν•΄ν•λ” λ° λ„μ›€μ„ μ¤€λ‹¤.

μ „μ²΄ μ½”λ“λ¥Ό λ³΄κ³  μ‹¶λ‹¤λ©΄ [ν΄λ¦­](https://github.com/bom1215/portfolio/blob/main/deep_learning_from_scratch/train_mnist_mlp.ipynb)






